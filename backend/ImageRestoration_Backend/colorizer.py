import torch
import torch.nn as nn
from PIL import Image
from torchvision import transforms
import numpy as np
import os
from skimage import color

# ===============================
# 1ï¸âƒ£ AI ë‹´ë‹¹ìž UNet ëª¨ë¸ ì •ì˜
# ===============================
class DoubleConv(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
        )
    def forward(self, x):
        return self.layers(x)

class UNet(nn.Module):
    def __init__(self, in_channels=1, out_channels=2, base_c=32):
        super().__init__()
        self.enc1 = DoubleConv(in_channels, base_c)
        self.enc2 = DoubleConv(base_c, base_c*2)
        self.enc3 = DoubleConv(base_c*2, base_c*4)
        self.enc4 = DoubleConv(base_c*4, base_c*8)

        self.pool = nn.MaxPool2d(2)
        self.up = nn.Upsample(scale_factor=2, mode='nearest')

        self.dec3 = DoubleConv(base_c*8 + base_c*4, base_c*4)
        self.dec2 = DoubleConv(base_c*4 + base_c*2, base_c*2)
        self.dec1 = DoubleConv(base_c*2 + base_c, base_c)

        self.final_conv = nn.Conv2d(base_c, out_channels, kernel_size=3, padding=1)
        self.activation = nn.Tanh()  # ì¶œë ¥ ë²”ìœ„ [-1, 1]

    def forward(self, x):
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool(e1))
        e3 = self.enc3(self.pool(e2))
        e4 = self.enc4(self.pool(e3))

        d3 = self.up(e4)
        d3 = torch.cat([d3, e3], dim=1)
        d3 = self.dec3(d3)

        d2 = self.up(d3)
        d2 = torch.cat([d2, e2], dim=1)
        d2 = self.dec2(d2)

        d1 = self.up(d2)
        d1 = torch.cat([d1, e1], dim=1)
        d1 = self.dec1(d1)

        out = self.final_conv(d1)
        out = self.activation(out)
        return out

# ===============================
# 2ï¸âƒ£ ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
# ===============================
def load_colorizer(model_path):
    model = UNet(in_channels=1, out_channels=2)
    if os.path.exists(model_path):
        checkpoint = torch.load(model_path, map_location="cpu")
        if "state_dict" in checkpoint:
            state_dict = {k.replace("model.", ""): v for k, v in checkpoint["state_dict"].items()}
            model.load_state_dict(state_dict, strict=False)
        else:
            model.load_state_dict(checkpoint, strict=False)
        print(f"âœ… UNet ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
    else:
        print(f"âš ï¸ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
    model.eval()
    return model

# ===============================
# 3ï¸âƒ£ ì´ë¯¸ì§€ ì»¬ëŸ¬í™” í•¨ìˆ˜ (í•™ìŠµ ì •ê·œí™” ë°˜ì˜)
# ===============================
def colorize_image(model, input_path, output_dir):
    # L ì±„ë„ í‘ë°± ì´ë¯¸ì§€
    image = Image.open(input_path).convert("L")
    transform = transforms.Compose([
        transforms.Resize((320, 320)),           # í•™ìŠµ ì‹œ target_size
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.0], std=[1.0])  # ì´ë¯¸ Lì€ [-1,1]ë¡œ í›„ì²˜ë¦¬ ì˜ˆì •
    ])
    l_tensor = transform(image).unsqueeze(0)
    
    # í•™ìŠµ ì‹œ ì •ê·œí™”: L [-1,1] ë²”ìœ„, ëª¨ë¸ ìž…ë ¥ ê·¸ëŒ€ë¡œ
    l_tensor = (np.array(image.resize((320,320)), dtype=np.float32) / 50.0) - 1.0
    l_tensor = torch.from_numpy(l_tensor).unsqueeze(0).unsqueeze(0)  # (1,1,H,W)

    with torch.no_grad():
        ab_output = model(l_tensor)  # UNet ì¶œë ¥ ab ì±„ë„ [-1,1]
        ab_output = ab_output[0].permute(1, 2, 0).cpu().numpy()
        ab_output = np.clip(ab_output, -1, 1)  # ì•ˆì •í™”
        ab_output = ab_output * 128            # [-128,128] ë²”ìœ„

    # L ì±„ë„ ë³µì›: í•™ìŠµ ì‹œ ì •ê·œí™”ì™€ ë°˜ëŒ€ë¡œ ì²˜ë¦¬
    l_channel = ((l_tensor[0,0].cpu().numpy() + 1.0) * 50.0)  # 0~100

    # Lab ì´ë¯¸ì§€ í•©ì¹˜ê¸°
    lab_image = np.zeros((320, 320, 3), dtype=np.float32)
    lab_image[..., 0] = l_channel
    lab_image[..., 1:] = ab_output

    # Lab -> RGB
    rgb_image = color.lab2rgb(lab_image)
    rgb_image = (rgb_image * 255).clip(0, 255).astype(np.uint8)
    output_image = Image.fromarray(rgb_image)

    # ì €ìž¥
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"colorized_{os.path.basename(input_path)}")
    output_image.save(output_path)
    print(f"ðŸŽ¨ ì»¬ëŸ¬í™” ì™„ë£Œ: {output_path}")
    return output_path
