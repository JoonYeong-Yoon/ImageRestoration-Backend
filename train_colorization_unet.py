"""
train_colorization_unet.py
- í‘ë°± â†’ ì»¬ëŸ¬í™” UNet í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
- í•™ìŠµ ì™„ë£Œ ì‹œ checkpoints_unet/ í´ë”ì— .ckpt íŒŒì¼ ìë™ ì €ì¥
"""

import os
import torch
import torchvision.transforms as T
from PIL import Image
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
import segmentation_models_pytorch as smp
from torchvision.utils import save_image


# ------------------------------------------------------------
# âœ… ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
# ------------------------------------------------------------
class ColorizationDataset(Dataset):
    def __init__(self, image_dir, size=128):
        self.image_dir = image_dir
        self.image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir)
                            if f.lower().endswith((".jpg", ".png", ".jpeg"))]
        self.transform_rgb = T.Compose([
            T.Resize((size, size)),
            T.ToTensor()
        ])
        self.transform_gray = T.Compose([
            T.Resize((size, size)),
            T.Grayscale(num_output_channels=1),
            T.ToTensor()
        ])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        img = Image.open(img_path).convert("RGB")

        y = self.transform_rgb(img)
        x = self.transform_gray(img)
        return x, y


# ------------------------------------------------------------
# âœ… Lightning ëª¨ë¸ ì •ì˜
# ------------------------------------------------------------
class LitColorization(pl.LightningModule):
    def __init__(self, lr=1e-4):
        super().__init__()
        self.model = smp.Unet(
            encoder_name="resnet34",
            encoder_weights="imagenet",
            in_channels=1,
            classes=3
        )
        self.loss_fn = nn.L1Loss()
        self.lr = lr
        self.activation = nn.Tanh()

    def forward(self, x):
        return self.activation(self.model(x))

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_fn(y_hat, y)
        self.log("train_loss", loss, prog_bar=True)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_fn(y_hat, y)
        self.log("val_loss", loss, prog_bar=True)
        return loss

    def configure_optimizers(self):
        optimizer = optim.Adam(self.parameters(), lr=self.lr)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode="min", factor=0.5, patience=3
        )
        return {"optimizer": optimizer, "lr_scheduler": scheduler, "monitor": "val_loss"}


# ------------------------------------------------------------
# âœ… ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
# ------------------------------------------------------------
if __name__ == "__main__":
    # í•™ìŠµìš© ì´ë¯¸ì§€ í´ë” ì§€ì •
    train_dir = r"E:\image_train"      # âš ï¸ ë„ˆì˜ ì´ë¯¸ì§€ ë°ì´í„° ê²½ë¡œë¡œ ë³€ê²½
    val_dir = r"E:\image_val"          # âš ï¸ ê²€ì¦ìš© ë°ì´í„° ê²½ë¡œë¡œ ë³€ê²½

    # í´ë” ì¡´ì¬ í™•ì¸
    if not os.path.exists(train_dir):
        raise FileNotFoundError(f"í•™ìŠµìš© í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {train_dir}")

    os.makedirs("checkpoints_unet", exist_ok=True)

    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = ColorizationDataset(train_dir)
    val_dataset = ColorizationDataset(val_dir)

    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)

    # ëª¨ë¸ ì´ˆê¸°í™”
    model = LitColorization()

    # ì½œë°± ì„¤ì •
    checkpoint_cb = ModelCheckpoint(
        dirpath="checkpoints_unet",
        filename="unet-{epoch:02d}-{val_loss:.4f}",
        monitor="val_loss",
        mode="min",
        save_top_k=3,
        save_last=True,
        verbose=True
    )

    early_stop = EarlyStopping(
        monitor="val_loss",
        patience=10,
        mode="min",
        verbose=True
    )

    # í•™ìŠµ ì‹¤í–‰
    trainer = pl.Trainer(
        max_epochs=30,
        accelerator="auto",
        devices=1,
        precision=32,
        callbacks=[checkpoint_cb, early_stop],
        log_every_n_steps=10
    )

    print("ğŸ§  ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    trainer.fit(model, train_loader, val_loader)
    print("âœ… í•™ìŠµ ì™„ë£Œ! ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ checkpoints_unet í´ë”ì— ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
