# -*- coding: utf-8 -*-
"""ImageRestoration_VAE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14zK08PTH96cuPJ9OKm6_tZ3iTf85gVYU
"""

# %% Imports
!pip install pytorch_lightning
import os, glob, argparse
from pathlib import Path
import numpy as np
from PIL import Image
import cv2

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor

# %% Dataset
class ColorizationDataset(Dataset):
    def __init__(self, file_list, target_size=320, augment=True):
        self.paths = file_list
        self.target_size = target_size
        self.augment = augment

        self.base_transform = transforms.Compose([
            transforms.Resize((target_size, target_size)),
            transforms.RandomHorizontalFlip() if augment else transforms.Lambda(lambda x: x),
            transforms.ToTensor(),
        ])

    def __len__(self):
        return len(self.paths)

    def __getitem__(self, idx):
        p = self.paths[idx]
        img = Image.open(p).convert('RGB')
        img_t = self.base_transform(img)
        img_np = (img_t.numpy() * 255).astype(np.uint8).transpose(1,2,0)

        lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB).astype(np.float32)
        L = lab[:,:,0:1]; ab = lab[:,:,1:]
        L = (L / 50.0) - 1.0
        ab = ab / 128.0

        Lt = torch.from_numpy(L.transpose(2,0,1).astype(np.float32))
        abt = torch.from_numpy(ab.transpose(2,0,1).astype(np.float32))
        return Lt, abt

# %% Model
class DoubleConv(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
        )
    def forward(self, x):
        return self.layers(x)

class UNet(nn.Module):
    def __init__(self, in_channels=1, out_channels=2, base_c=32):
        super().__init__()
        self.enc1 = DoubleConv(in_channels, base_c)
        self.enc2 = DoubleConv(base_c, base_c*2)
        self.enc3 = DoubleConv(base_c*2, base_c*4)
        self.enc4 = DoubleConv(base_c*4, base_c*8)

        self.pool = nn.MaxPool2d(2)
        self.up = nn.Upsample(scale_factor=2, mode='nearest')

        self.dec3 = DoubleConv(base_c*8 + base_c*4, base_c*4)
        self.dec2 = DoubleConv(base_c*4 + base_c*2, base_c*2)
        self.dec1 = DoubleConv(base_c*2 + base_c, base_c)

        self.final_conv = nn.Conv2d(base_c, out_channels, kernel_size=3, padding=1)
        self.activation = nn.Tanh()

    def forward(self, x):
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool(e1))
        e3 = self.enc3(self.pool(e2))
        e4 = self.enc4(self.pool(e3))

        d3 = self.up(e4)
        d3 = torch.cat([d3, e3], dim=1)
        d3 = self.dec3(d3)

        d2 = self.up(d3)
        d2 = torch.cat([d2, e2], dim=1)
        d2 = self.dec2(d2)

        d1 = self.up(d2)
        d1 = torch.cat([d1, e1], dim=1)
        d1 = self.dec1(d1)

        out = self.final_conv(d1)
        out = self.activation(out)
        return out

# %% Lightning Module
class LitColorization(pl.LightningModule):
    def __init__(self, hparams):
        super().__init__()
        self.save_hyperparameters(hparams)
        self.model = UNet(in_channels=1, out_channels=2, base_c=hparams.base_c)
        self.loss_fn = nn.L1Loss()

    def forward(self, L):
        return self.model(L)

    def training_step(self, batch, batch_idx):
        L, ab = batch
        pred_ab = self(L)
        loss = self.loss_fn(pred_ab, ab)
        self.log('train_loss', loss, on_epoch=True)
        return loss

    def validation_step(self, batch, batch_idx):
        L, ab = batch
        pred_ab = self(L)
        loss = self.loss_fn(pred_ab, ab)
        self.log('val_loss', loss, on_epoch=True)
        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)
        scheduler = {
            'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3),
            'monitor': 'val_loss'
        }
        return {'optimizer': optimizer, 'lr_scheduler': scheduler}

# %% Argument Parser
def build_argparser():
    p = argparse.ArgumentParser()
    p.add_argument('--data_dir', type=str, default="/content/drive/MyDrive/Advanced_Project/imagenette2-320")
    p.add_argument('--batch_size', type=int, default=16)
    p.add_argument('--max_epochs', type=int, default=30)
    p.add_argument('--lr', type=float, default=1e-4)
    p.add_argument('--base_c', type=int, default=32)
    p.add_argument('--gpus', type=int, default=1)
    p.add_argument('--num_workers', type=int, default=2)
    p.add_argument('--checkpoint_dir', type=str, default='checkpoints')
    p.add_argument('--output_dir', type=str, default='outputs')
    p.add_argument('--patience', type=int, default=6)
    return p

# %%
import sys

def main():
    # Colab/Jupyter에서 불필요한 argv 제거
    args = build_argparser().parse_args(args=[])

    pl.seed_everything(42)

    train_files = glob.glob(os.path.join(args.data_dir, "train/*/*.JPEG"))
    val_files   = glob.glob(os.path.join(args.data_dir, "val/*/*.JPEG"))

    if len(train_files) == 0 or len(val_files) == 0:
        raise RuntimeError("No training/validation images found! Check dataset path.")

    train_ds = ColorizationDataset(train_files, target_size=320, augment=True)
    val_ds = ColorizationDataset(val_files, target_size=320, augment=False)

    train_dl = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True,
                          num_workers=args.num_workers, persistent_workers=True)
    val_dl = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False,
                        num_workers=args.num_workers, persistent_workers=True)

    model = LitColorization(args)

    ckpt_cb = ModelCheckpoint(dirpath=args.checkpoint_dir,
                              filename='best-{epoch:02d}-{val_loss:.4f}',
                              monitor='val_loss', mode='min', save_top_k=3)
    es_cb = EarlyStopping(monitor='val_loss', patience=args.patience, mode='min')
    lr_mon = LearningRateMonitor(logging_interval='epoch')

    trainer = pl.Trainer(
        max_epochs=args.max_epochs,
        accelerator="gpu" if torch.cuda.is_available() else "cpu",
        devices=1 if torch.cuda.is_available() else None,
        callbacks=[ckpt_cb, es_cb, lr_mon],
        default_root_dir='.',
        log_every_n_steps=50,
    )

    trainer.fit(model, train_dl, val_dl)

if __name__ == "__main__":
    main()

# %%
!ls checkpoints/

# %%
"""
Colab용: U-Net Colorization Test Script
"""

# %%
import os, glob
from pathlib import Path
from PIL import Image
import numpy as np
import cv2
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import pytorch_lightning as pl
from pytorch_lightning import LightningModule

# %%
# ------------------------------
# Dataset
# ------------------------------
class ColorizationDataset(Dataset):
    def __init__(self, file_list, target_size=320):
        self.paths = file_list
        self.target_size = target_size
        self.transform = transforms.Compose([
            transforms.Resize((target_size, target_size)),
            transforms.ToTensor()
        ])
    def __len__(self):
        return len(self.paths)
    def __getitem__(self, idx):
        p = self.paths[idx]
        img = Image.open(p).convert("RGB")
        img_t = self.transform(img)
        img_gray = img.convert("L")
        gray_t = self.transform(img_gray)
        return gray_t, img_t, p  # grayscale, original color, path

# %%
# ------------------------------
# U-Net Model (same as training)
# ------------------------------
class DoubleConv(torch.nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.layers = torch.nn.Sequential(
            torch.nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),
            torch.nn.BatchNorm2d(out_ch),
            torch.nn.ReLU(inplace=True),
            torch.nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),
            torch.nn.BatchNorm2d(out_ch),
            torch.nn.ReLU(inplace=True)
        )
    def forward(self, x):
        return self.layers(x)

class UNet(torch.nn.Module):
    def __init__(self, in_channels=1, out_channels=2, base_c=32):
        super().__init__()
        self.enc1 = DoubleConv(in_channels, base_c)
        self.enc2 = DoubleConv(base_c, base_c*2)
        self.enc3 = DoubleConv(base_c*2, base_c*4)
        self.enc4 = DoubleConv(base_c*4, base_c*8)
        self.pool = torch.nn.MaxPool2d(2)
        self.up = torch.nn.Upsample(scale_factor=2, mode='nearest')
        self.dec3 = DoubleConv(base_c*8 + base_c*4, base_c*4)
        self.dec2 = DoubleConv(base_c*4 + base_c*2, base_c*2)
        self.dec1 = DoubleConv(base_c*2 + base_c, base_c)
        self.final_conv = torch.nn.Conv2d(base_c, out_channels, kernel_size=3, padding=1)
        self.activation = torch.nn.Tanh()
    def forward(self, x):
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool(e1))
        e3 = self.enc3(self.pool(e2))
        e4 = self.enc4(self.pool(e3))
        d3 = self.up(e4)
        d3 = torch.cat([d3, e3], dim=1)
        d3 = self.dec3(d3)
        d2 = self.up(d3)
        d2 = torch.cat([d2, e2], dim=1)
        d2 = self.dec2(d2)
        d1 = self.up(d2)
        d1 = torch.cat([d1, e1], dim=1)
        d1 = self.dec1(d1)
        out = self.final_conv(d1)
        out = self.activation(out)
        return out

# %%
# ------------------------------
# Lightning Module
# ------------------------------
class LitColorization(pl.LightningModule):
    def __init__(self, hparams):
        super().__init__()
        # hparams는 단순 참조용만 사용
        if isinstance(hparams, dict):
            import argparse
            hparams = argparse.Namespace(**hparams)
        self.model = UNet(in_channels=1, out_channels=2, base_c=hparams.base_c)

    def forward(self, L):
        return self.model(L)

# %%
# ------------------------------
# LAB -> RGB
# ------------------------------
def lab_to_rgb(L_tensor, ab_tensor):
    L = L_tensor.detach().cpu().numpy()
    ab = ab_tensor.detach().cpu().numpy()
    N = L.shape[0]
    out_rgb = []
    for i in range(N):
        Li = (L[i,0,:,:] + 1.0) * 50.0
        abi = ab[i,:,:,:].transpose(1,2,0) * 128.0
        lab = np.zeros((Li.shape[0], Li.shape[1], 3), dtype=np.float32)
        lab[:,:,0] = Li
        lab[:,:,1:] = abi
        rgb = cv2.cvtColor(lab.astype(np.uint8), cv2.COLOR_LAB2RGB)
        rgb = np.clip(rgb.astype(np.float32)/255.0,0,1)
        out_rgb.append(torch.from_numpy(rgb.transpose(2,0,1).astype(np.float32)))
    return torch.stack(out_rgb, dim=0)

# %%
# ------------------------------
# Main Test
# ------------------------------
def test_colorization(ckpt_path, test_dir, output_dir="/content/outputs"):
    os.makedirs(output_dir, exist_ok=True)
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # load checkpoint
    model = LitColorization.load_from_checkpoint(ckpt_path)
    model.to(device)
    model.eval()

    # load test images
    test_files = glob.glob(os.path.join(test_dir, "*/*.JPEG"))
    dataset = ColorizationDataset(test_files)
    loader = DataLoader(dataset, batch_size=1, shuffle=False)

    for gray, color_orig, path in loader:
        gray = gray.to(device)
        with torch.no_grad():
            pred_ab = model(gray)
        rgb_pred = lab_to_rgb(gray, pred_ab)[0]
        # save
        save_path = os.path.join(output_dir, Path(path[0]).name)
        arr = (rgb_pred.cpu().numpy().transpose(1,2,0) * 255).astype(np.uint8)
        Image.fromarray(arr).save(save_path)
        print(f"Saved: {save_path}")

# %%
# ------------------------------
# Usage
# ------------------------------
if __name__ == "__main__":
    ckpt_path = "/content/checkpoints/best-epoch=29-val_loss=0.0693.ckpt"  # 예시 경로
    test_dir = "/content/drive/MyDrive/Advanced_Project/imagenette2-320/val"
    test_colorization(ckpt_path, test_dir)

# %%
import torch
from PIL import Image
import numpy as np
import cv2
from torchvision import transforms
import matplotlib.pyplot as plt

# %%
# ------------------------------
# Helpers
# ------------------------------

def preprocess_image(img_path, target_size=320):
    """이미지 불러와서 L 채널 텐서로 변환"""
    img = Image.open(img_path).convert('RGB')
    transform = transforms.Compose([
        transforms.Resize((target_size, target_size)),
        transforms.ToTensor(),
    ])
    img_t = transform(img)
    img_np = (img_t.numpy() * 255).astype(np.uint8).transpose(1,2,0)
    lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB).astype(np.float32)
    L = lab[:,:,0:1]
    L = (L / 50.0) - 1.0
    Lt = torch.from_numpy(L.transpose(2,0,1).astype(np.float32))
    Lt = Lt.unsqueeze(0)  # 배치 차원 추가
    return Lt, img

def lab_tensor_to_rgb(L_tensor, ab_tensor):
    """L, ab 텐서를 RGB 이미지로 변환"""
    L = L_tensor.detach().cpu().numpy()
    ab = ab_tensor.detach().cpu().numpy()
    Li = (L[0,0,:,:] + 1.0) * 50.0
    abi = ab[0].transpose(1,2,0) * 128.0
    lab = np.zeros((Li.shape[0], Li.shape[1], 3), dtype=np.float32)
    lab[:,:,0] = Li
    lab[:,:,1:] = abi
    rgb = cv2.cvtColor(lab.astype(np.uint8), cv2.COLOR_LAB2RGB)
    rgb = np.clip(rgb.astype(np.float32) / 255.0, 0.0, 1.0)
    return rgb

# %%
# ------------------------------
# 테스트 + 시각화
# ------------------------------
def visualize_multiple_ckpts(ckpt_paths, img_path, device='cuda'):
    # 입력 전처리
    L_tensor, orig_img = preprocess_image(img_path)
    L_tensor = L_tensor.to(device)

    # 흑백 이미지 변환
    L_np = ((L_tensor[0,0].cpu().numpy() + 1.0) * 50.0).astype(np.uint8)
    L_gray = cv2.cvtColor(np.stack([L_np]*3, axis=-1), cv2.COLOR_LAB2RGB)
    L_gray = np.clip(L_gray.astype(np.float32)/255.0, 0, 1)

    # 시각화 설정
    num_ckpts = len(ckpt_paths)
    plt.figure(figsize=(5*(num_ckpts+1), 5))

    # 좌측: 흑백 입력
    plt.subplot(1, num_ckpts+1, 1)
    plt.imshow(L_gray)
    plt.title("Input Grayscale")
    plt.axis('off')

    # 각 체크포인트별 복원
    for i, ckpt_path in enumerate(ckpt_paths):
        model = LitColorization.load_from_checkpoint(ckpt_path)
        model.to(device)
        model.eval()
        with torch.no_grad():
            pred_ab = model(L_tensor)
        rgb = lab_tensor_to_rgb(L_tensor, pred_ab)
        plt.subplot(1, num_ckpts+1, i+2)
        plt.imshow(rgb)
        plt.title(f"Restored Color\nCheckpoint {i+1}")
        plt.axis('off')

    plt.show()

# %%
# ------------------------------
# 실행 예시
# ------------------------------
if __name__ == "__main__":
    ckpt_paths = [
        "/content/checkpoints/best-epoch=23-val_loss=0.0695.ckpt",
        "/content/checkpoints/best-epoch=28-val_loss=0.0695.ckpt",
        "/content/checkpoints/best-epoch=29-val_loss=0.0693.ckpt",
    ]
    img_path = "/content/drive/MyDrive/Advanced_Project/TEST6.JPEG"
    visualize_multiple_ckpts(ckpt_paths, img_path, device='cuda' if torch.cuda.is_available() else 'cpu')
    
    
    #colab 버전이라서 내가 사용하는 버전으로 전체 바꾸기 